# Mem貌ria T猫cnica - Crawler de Detecci贸 d'Errors 4XX

## Descripci贸 del Projecte
Aquest projecte consisteix en un crawler desenvolupat en Python que explora un domini web i detecta totes les URLs que retornin codis d'error 4XX. El programa utilitza Selenium per navegar per les pgines web i BeautifulSoup per extreure els enlla莽os interns. A m茅s, s'usa la llibreria `requests` per comprovar l'estat HTTP de cada URL visitada.

## Tecnologies Utilitzades
- **Python** (llenguatge principal)
- **Selenium** (automatitzaci贸 de la navegaci贸 web)
- **BeautifulSoup** (extracci贸 d'enlla莽os d'HTML)
- **requests** (comprovaci贸 de l'estat de les URLs)
- **CSV** (generaci贸 d'informe d'errors)

## Proc茅s de Disseny i Decisions Preses
### 1. Inicialitzaci贸 del Navegador
S'utilitza Selenium per carregar les pgines web. S'usa `webdriver_manager` per instal路lar automticament el driver de Chrome:
```python
def inicia_navegador():
    options = webdriver.ChromeOptions()
    navegador = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)
    return navegador
```

### 2. Extracci贸 d'Enlla莽os
S'analitza el codi HTML amb BeautifulSoup per trobar enlla莽os interns:
```python
def obtenir_enllacos_interns(navegador, url_base):
    navegador.get(url_base)
    time.sleep(2)
    codi_html = navegador.page_source
    soup = BeautifulSoup(codi_html, 'html.parser')
    enllacos = set()
    for etiqueta_a in soup.find_all('a', href=True):
        enllac = etiqueta_a['href']
        if enllac.startswith('/'):
            enllac = urljoin(url_base, enllac)
        if urlparse(enllac).netloc == urlparse(url_base).netloc:
            enllacos.add(enllac)
    return enllacos
```

### 3. Comprovaci贸 de l'Estat de les URLs
Cada URL visitada 茅s verificada per detectar si retorna un codi 4XX:
```python
def comprovar_estat_url(url):
    try:
        resposta = requests.get(url)
        return resposta.status_code
    except requests.exceptions.RequestException:
        return None
```

### 4. L铆mit de Pgines Visitades
Per evitar que el crawler es quedi encallat o explori indefinidament, s'ha establert un l铆mit de 10 pgines:
```python
max_pagines = 10
if comptador >= max_pagines:
    print(f" S'ha assolit el l铆mit de {max_pagines} pgines. Aturant...")
    break
```

### 5. Generaci贸 de l'Informe
Els errors trobats es guarden en un fitxer CSV:
```python
def generar_informe(errors_4xx, nom_fitxer='informe_errors_4xx.csv'):
    claus = errors_4xx[0].keys() if errors_4xx else []
    with open(nom_fitxer, mode='w', newline='', encoding='utf-8') as fitxer:
        escriptor = csv.DictWriter(fitxer, fieldnames=claus)
        escriptor.writeheader()
        escriptor.writerows(errors_4xx)
```

## Com Utilitzar el Crawler
### 1. Instal路laci贸 de Depend猫ncies
Executa la seg眉ent comanda per instal路lar els paquets necessaris:
```bash
pip install selenium beautifulsoup4 requests webdriver_manager
```

### 2. Execuci贸 del Programa
Executa el crawler amb:
```bash
python crawler.py
```
El programa explorar el domini especificat i generar un informe `informe_errors_4xx.csv` amb els errors trobats.

## Conclusions
Aquest crawler permet identificar errors 4XX dins d'un domini web de manera eficient, fent servir Selenium per la navegaci贸 i `requests` per la verificaci贸 d'estats HTTP. L'estrat猫gia de limitaci贸 de pgines evita cicles infinits i millora el rendiment. El projecte pot ampliar-se en el futur per incloure funcionalitats com la detecci贸 d'altres codis d'error HTTP o l'extracci贸 de m茅s metadades de cada pgina.

